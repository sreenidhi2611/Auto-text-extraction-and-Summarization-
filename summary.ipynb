{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "70d69b27cc51489496b24ef17420579a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa68f43afb844a4b8e528c88b5d2b5c3",
              "IPY_MODEL_c5cdc335948b4f94b82f7e5d0569c88e",
              "IPY_MODEL_2498ba2aa1fd466e903f14305708b1aa"
            ],
            "layout": "IPY_MODEL_19e31796143746cf9837c8faad70d05e"
          }
        },
        "aa68f43afb844a4b8e528c88b5d2b5c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2331bf62a8364773a31e60eea1088908",
            "placeholder": "​",
            "style": "IPY_MODEL_353336ad0b5c4e738680fb482fb25552",
            "value": "config.json: 100%"
          }
        },
        "c5cdc335948b4f94b82f7e5d0569c88e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7ac82be636b4421bd06d5186397be35",
            "max": 1802,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ae0fbc1628d46098017d16bcfe396e4",
            "value": 1802
          }
        },
        "2498ba2aa1fd466e903f14305708b1aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3fbaae2bf974fd989d7b82039b27fc6",
            "placeholder": "​",
            "style": "IPY_MODEL_1641ee8871824fea93ee31e33b93840a",
            "value": " 1.80k/1.80k [00:00&lt;00:00, 75.5kB/s]"
          }
        },
        "19e31796143746cf9837c8faad70d05e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2331bf62a8364773a31e60eea1088908": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "353336ad0b5c4e738680fb482fb25552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7ac82be636b4421bd06d5186397be35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ae0fbc1628d46098017d16bcfe396e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3fbaae2bf974fd989d7b82039b27fc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1641ee8871824fea93ee31e33b93840a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eaf085d27a144f68071229ccd296321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3c0f5e326c24417945110d7534a2aa8",
              "IPY_MODEL_8f68fe0c857b44fc82813cc6e5b41a6a",
              "IPY_MODEL_1c7856f8af5b4972b0bc3436f57ed5b9"
            ],
            "layout": "IPY_MODEL_f7343b9120054ba89a792e4926e16246"
          }
        },
        "f3c0f5e326c24417945110d7534a2aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fbfb6750dcd4e6a808fe15472c99f5c",
            "placeholder": "​",
            "style": "IPY_MODEL_618137c0c2ef4f2c85c528b32fba5d16",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "8f68fe0c857b44fc82813cc6e5b41a6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_136197d14f6d4585ba8bd623aa2ed171",
            "max": 1222317369,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1dbb599f05554b00a3c55da3348eddb5",
            "value": 1222317369
          }
        },
        "1c7856f8af5b4972b0bc3436f57ed5b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73df74e5fdc741ef9c937f8fa0c32be0",
            "placeholder": "​",
            "style": "IPY_MODEL_c13ee32713f3424994f599d84d5ba7dd",
            "value": " 1.22G/1.22G [00:11&lt;00:00, 145MB/s]"
          }
        },
        "f7343b9120054ba89a792e4926e16246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fbfb6750dcd4e6a808fe15472c99f5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "618137c0c2ef4f2c85c528b32fba5d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "136197d14f6d4585ba8bd623aa2ed171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dbb599f05554b00a3c55da3348eddb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73df74e5fdc741ef9c937f8fa0c32be0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c13ee32713f3424994f599d84d5ba7dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "291a2aac0cdc4b04b00920bab6264195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf135fdf2c8d4631a808b5feb0b395f3",
              "IPY_MODEL_1de8d27f1ecb44bbb24f8909c9a75f0a",
              "IPY_MODEL_b58b02ecb4e84aef82646ef0c5e1da53"
            ],
            "layout": "IPY_MODEL_c2122bfe494b4235956433c0d3a5a426"
          }
        },
        "cf135fdf2c8d4631a808b5feb0b395f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45205801bceb4c768057d18f7303279a",
            "placeholder": "​",
            "style": "IPY_MODEL_78ff0fe7a7a348909323bcb501c7cd73",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "1de8d27f1ecb44bbb24f8909c9a75f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_801cac2f51264417ad3c65d7ea1e3ac6",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b89b6d1218b74476ac15c316cd95a4a2",
            "value": 26
          }
        },
        "b58b02ecb4e84aef82646ef0c5e1da53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b982f9cac264427b65666a155f67116",
            "placeholder": "​",
            "style": "IPY_MODEL_2156b39998924ea08b7d229746061ecc",
            "value": " 26.0/26.0 [00:00&lt;00:00, 1.73kB/s]"
          }
        },
        "c2122bfe494b4235956433c0d3a5a426": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45205801bceb4c768057d18f7303279a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78ff0fe7a7a348909323bcb501c7cd73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "801cac2f51264417ad3c65d7ea1e3ac6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b89b6d1218b74476ac15c316cd95a4a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b982f9cac264427b65666a155f67116": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2156b39998924ea08b7d229746061ecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72c3ae60162747caade1cbd080ba8542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a672974680f04055a3c6c6c3f3d9c065",
              "IPY_MODEL_4413c9423b944fae8493a50bbfc784ac",
              "IPY_MODEL_1f656c9e3b564b9bbc017ac1cb1c8f80"
            ],
            "layout": "IPY_MODEL_e76ddba5ceb74b2ca58a88eb188db05a"
          }
        },
        "a672974680f04055a3c6c6c3f3d9c065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4187921a332140ffafe1145ee77e54a2",
            "placeholder": "​",
            "style": "IPY_MODEL_181403172b5548758959117549ff595a",
            "value": "vocab.json: 100%"
          }
        },
        "4413c9423b944fae8493a50bbfc784ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1dc2f7d605c43c7b50b6a3725de60aa",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba35f0a3b7af4b0c96515ccd1aea09c6",
            "value": 898822
          }
        },
        "1f656c9e3b564b9bbc017ac1cb1c8f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2a7512d7acf4c03bb5adf0ed0b7977b",
            "placeholder": "​",
            "style": "IPY_MODEL_0c41851fb7214317bef2b87bf374e95a",
            "value": " 899k/899k [00:00&lt;00:00, 6.45MB/s]"
          }
        },
        "e76ddba5ceb74b2ca58a88eb188db05a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4187921a332140ffafe1145ee77e54a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "181403172b5548758959117549ff595a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1dc2f7d605c43c7b50b6a3725de60aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba35f0a3b7af4b0c96515ccd1aea09c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2a7512d7acf4c03bb5adf0ed0b7977b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c41851fb7214317bef2b87bf374e95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "644a910a88ff4bf4bbce188adbfdbd68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_acfe3d24e7e4456cb917ff1a24ea43d4",
              "IPY_MODEL_80b7a8c4924f4bccb837042f5e0c763b",
              "IPY_MODEL_2cc528144764444da10ea89f3cbd60ed"
            ],
            "layout": "IPY_MODEL_bf63605e927f4b0f8b9542e5850796f8"
          }
        },
        "acfe3d24e7e4456cb917ff1a24ea43d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1668209ddde42a3ba7104183194e0b7",
            "placeholder": "​",
            "style": "IPY_MODEL_4111bb583ddb4f838f5df60a42525471",
            "value": "merges.txt: 100%"
          }
        },
        "80b7a8c4924f4bccb837042f5e0c763b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2603feda5c4a49308dd9c1d3f0391e0d",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_825a772a264d46ab836e415b220853ed",
            "value": 456318
          }
        },
        "2cc528144764444da10ea89f3cbd60ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1abc7cff59a5403d951d4e20d490eea3",
            "placeholder": "​",
            "style": "IPY_MODEL_888da28c32d343b7b91830546dc0816f",
            "value": " 456k/456k [00:00&lt;00:00, 2.32MB/s]"
          }
        },
        "bf63605e927f4b0f8b9542e5850796f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1668209ddde42a3ba7104183194e0b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4111bb583ddb4f838f5df60a42525471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2603feda5c4a49308dd9c1d3f0391e0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "825a772a264d46ab836e415b220853ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1abc7cff59a5403d951d4e20d490eea3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "888da28c32d343b7b91830546dc0816f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29VL2JxkRyQR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEpjnNQdRzw6",
        "outputId": "25f463b9-1031-4489-c4f9-bbcabcd22cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Input text to be summarized\n",
        "text = \"\"\"\n",
        "Detection  of fire  using Deep Learning  models\n",
        "Sinchana R1, Sreenidhi K1, Mohith Krishna M Kamath1,\n",
        " Jeshma Nishitha D’Souza2\n",
        "\n",
        "1Department of Advanced Computing, St. Joseph’s University,\n",
        "36, Langford Road, Bengaluru -560027\n",
        "sinchanar21@gmail.com , karanamsreenidhiramprasad@gmail.com ,\n",
        "mohithkrishnakamath@gmail.com\n",
        "2Department of Advanced Computing, St. Joseph’s University,\n",
        "36, Langford Road, Bengaluru -560027\n",
        "jeshma.dsouza@sju.edu.in\n",
        "\n",
        "Abstract.  Fire acc idents are among the incidents posing a significant threat to\n",
        "valuable assets. They can cause damage economically, financially and\n",
        "materially wealth, and lead to the depletion of natural resources like forests and\n",
        "jeopardize human lives. In this paper, we have developed deep learning models\n",
        "that assist in detecting fires in various environments like forests, stores,\n",
        "commercial kitchens, meeting rooms, etc. We have successfully demonstrated\n",
        "the effectiveness of de ep lear ning models for fire detection using a  hybrid\n",
        "model called CNN -LSTM (Convolutional Neural Networks Long Short -Term\n",
        "Memory). However, it is worth mentioning that training these models can be\n",
        "computationally intensive and require large datasets. Additionally, our study\n",
        "suggests combining MobileNetV2 and Xception architectures to form an\n",
        "ensemble of Convolutional Neural Networks (CNNs) for detecting fires. This\n",
        "paper proposes utilizing the Xception models alongside MobileNetV2\n",
        "architectures to create an  ensemb le, for fire detection purposes. Exper imental\n",
        "results show that the ensemble of  Xception, ResNet50, I nception gave a best\n",
        "result  of 94%.\n",
        "Keywords: Fire detection, Deep Learning, CNN -LSTM hybrid model,\n",
        "Ensemble models, Xception, MobileNetV2,\n",
        "1.   Introduction\n",
        "Fire, one of the oldest discoveries in our evolutionary journey, has served as both a\n",
        "powerful friend, bringing warmth, protection, and culinary inventiveness, and a\n",
        "terrible opponent, unleashing disaster when left unchecked. Fire accidents have  been a\n",
        "relentless and ever -present threat in  the story of human existence. The nature and\n",
        "causes of fire accidents evolved alongside our understanding and mastery of fire,\n",
        "reflecting the developing complexities of our societies and technologies. As varied as\n",
        "the flames themselves are the reasons behind fire accidents. They range from\n",
        "inadvertent causes like electrical malfunctions, culinary accidents, or heating system\n",
        "breakdowns —which are frequently caused by mistakes made by people or broken\n",
        "equipment —to unpred ictable natural disasters like lightni ng strikes, volcanic\n",
        "eruptions, and wildfires. Deliberate acts of arson also highlight the potentially\n",
        "destructive nature of human intent, which adds another level of intricacy to this age -\n",
        "old problem.\n",
        "Fire accidents cause more than just immediate destruction property damage and\n",
        "fatalities are only the beginning they have the power to destroy ecosystems and\n",
        "unleash contaminants into the surrounding environment long -term health concerns\n",
        "and respiratory difficultie s are f aced by survivors they put a pressure on economies\n",
        "through insurance claims reconstruction costs and commercial disruptions the\n",
        "psychological cost which is sometimes overlooked consists of anxiety and anguish\n",
        "communication transportation and power are all impacted when essential\n",
        "infrastructure is disrupted .[1]\n",
        "By using a variety of technologies, safety precautions have been greatly enhanced by\n",
        "the detection and prevention of fire occurrences. Heat sensors, smoke detectors, and\n",
        "sophisticated fire supp ression systems are important examples of these [ 3]. These\n",
        "\n",
        "technologies, however, are not perfect and have flaws that can limit their usefulness in\n",
        "specific situations. Smoke detectors are essential for detecting fires, however they can\n",
        "malfunction in low -smoke or slow -burning fires. Their placement also affects how\n",
        "efficient they are, thus they are not appropriate for outdoor or highly humid areas.\n",
        "Low-heat or slow -burning fires may be difficult for heat sensors to detect, especially\n",
        "in the early phases of the fire. Heat sensors are made to detect tem peratu re spikes\n",
        "during fires [ 5]. While automatic sprinklers and fire -extinguishing agents are\n",
        "examples of advanced fire suppression systems that are generally dependable, they\n",
        "can malfunction as a result of improper maintenance. Water -based systems'\n",
        "effectiveness may be limited if they are used in situations where flammable\n",
        "substances, electrical equipment, or particular chemical reactions are involved.\n",
        "\n",
        "In the recent past, AI -enabled surveillance technologies h ave overcome a number of\n",
        "shortcomings; they r epresent a substantial advanceme nt over traditional methods —\n",
        "heat sensors in particular. AI surveillance is flexible for threat identification since, in\n",
        "contrast to traditional sensors, it is highly proficient in identifying a broad range of\n",
        "objects, behaviours , and anomalies [ 9]. It also reacts to minute variations in\n",
        "movement and temperature, which improves the early identification of possible\n",
        "threats. These systems can monitor vast regions effectively due to th eir scalability and\n",
        "capacity to analyse data on their own, eliminating the need for continual human\n",
        "supervision. Additionally, by using data analysis to improve decision -making, AI\n",
        "surveillance helps with resource allocation and security threat prediction. Concerns\n",
        "are addressed by privacy protection measures, and integrat ion with other security\n",
        "systems builds a full security ecosystem. Thus, we see that AI -enabled surveillance\n",
        "effectively overcomes the drawbacks of previous techniques to provide a more\n",
        "flexible, effective, and all -encompassing approa ch to security and surveillance.\n",
        "\n",
        "This paper uses cutting -edge models and tools for AI -enabled fire detection, such as a\n",
        "hybrid CNN -LSTM model and a combination of the Xception and MobileNetV2\n",
        "models. The Xception and MobileNetV2 fusion combine the advantages of these\n",
        "well-known  deep learning models in an effort to enhance fire detection. Furthermore,\n",
        "to efficiently analyse static images and temporal patterns, the CNN -LSTM hybrid\n",
        "model combines Convolutional Neur al Networks (CNNs) with Long Short -Term\n",
        "Memor y (LSTM) networks. The accuracy of fire detection could be improved by\n",
        "using this method. In order to improve the accuracy and efficacy of fire detection\n",
        "systems, this research assesses these models using fire image datasets. By integrating\n",
        "well-established deep learning archite ctures, these novel approaches have the\n",
        "potential to advance the field of fire detection technologies.\n",
        "\n",
        "This proactive strategy has the potential to greatly increase safety by facilitating  quick\n",
        "reactions and reducing the possibility  of loss of life and resources. AI technology\n",
        "added to surveillance cameras increases security in a number of ways, including\n",
        "behaviour analysis and anomaly identification - in addition to fire detection. This\n",
        "renders it an all -encompassing and adaptable resolution for enhancing  safety in a\n",
        "variety  of contexts.\n",
        "\n",
        "2.   Related Works:\n",
        "Data from several sources, including Google, Kaggle, satellite imagery, and the\n",
        "BoWFire dataset to create a dataset of 6,911 photos of  smoke and forest fires for their\n",
        "study. Desp ite having few samples in their target domain, they used the power of\n",
        "transfer learning to classify these photos by using pre -existing information from the\n",
        "ImageNet dataset. With VGG16 reaching 93.2% accuracy, InceptionV3 at 94.5%, and\n",
        "Xception leading at 95.1%, this method prod uced encouraging results. The use of a\n",
        "diversified dataset, improved model accuracy, and the application of the \"Learning\n",
        "without Forgetting\" (LwF) principle to prevent the loss of prior i nformation are all\n",
        "advantages of their approaches. The  risk of overfitting the models to the training data,\n",
        "which may compromise their performance on new data, as well as the computing\n",
        "requirements of developing and deploying these sophisticated models are potential\n",
        "downsides.[1]\n",
        "\n",
        "Convolutional Neural Networks (CNNs) have b een highlighted as a viable approach\n",
        "in recent research on image -based fire detection. Prior studies, on the other hand,\n",
        "mostly assessed CNNs on balanced datasets, which might not accurate ly represent\n",
        "real-world settings when fires a re uncommon. This paper presents a novel approach\n",
        "for fire detection employing even deeper CNNs, VGG16 and Resnet50, optimized\n",
        "with completely connected layers. Their analysis uses a dataset that is intentionally\n",
        "skewed to imitate real -world problems; non -fire photographs predom inate over fire\n",
        "images. The results show that adding fully connected layers for fine -tuning increases\n",
        "accuracy, even though it requires more training time. These  deeper CNNs display\n",
        "commend able performance on a more realistic dataset,  with Resnet50 narrowly\n",
        "beating out VGG16. Although there will still be a trade -off in terms of longer training\n",
        "sessions, these results point toward more effective fire detection systems.[2]\n",
        "\n",
        "In this investigation, researchers concentrated on the creation and assessment of Fire -\n",
        "Net, a deep learning system built to identify current fires from Landsat -8 imagery.\n",
        "They used a two -stream pixel classification approach with a distinctive design to\n",
        "improve the detection performance. The identification of current fi res was handled by\n",
        "one stream, while background and non -fire items were handled by the other. This\n",
        "method addresses the problem of imbalanced datasets by simulating actual conditions\n",
        "when fires are seldom. Preprocessing of the Landsat -8 photos included\n",
        "orthorectification and rad iometric correction. The deep learning architecture used\n",
        "residual blocks to address gradient problems and multi -scale residual convolution\n",
        "layers to ensure robustness against scale fluctua tions. A hybrid loss function and the\n",
        "Adam op timizer were used to train the network using a set of hyperparameters.\n",
        "Visual analysis and the computation of multiple metrics, such as recall, overall\n",
        "accuracy, precision, and F1 -score, were part of the evaluation process. The model's\n",
        "performance was evaluated against a variet y of machine learning algorithms,\n",
        "including Random Forest, K -Nearest Neighbors, Support Vector Machine, Multi -\n",
        "Layer Perceptron, and Extreme Gradient Boosting, in addition to the well -known\n",
        "CNN, MSR -U-Net. The robustness and efficacy of this method in identifying small\n",
        "fires are its advantages; however, the need for meticulous hyperparameter tuning and\n",
        "extended training periods are possible disadvantages.[3]\n",
        "\n",
        "To address the shortcomings of conventional computer vision -based fire detection\n",
        "strategies that rel y on static and short -term temporal characteristics, a novel deep\n",
        "learning -based fire detection method called DTA (Decision Through Aggregation)\n",
        "was suggested in this study. For increased accuracy, the DTA approach closely\n",
        "resembles human decision -making processes. There are three main elements to the\n",
        "architecture.  First, suspected regions of fire (SRoFs) and non -fire objects are\n",
        "\n",
        "distinguished in video frames using the Faster R -CNN deep object detection model.\n",
        "Bounding boxes are used to capture different cl asses, such as smoke, flames, and non -\n",
        "fire items. After that, the spatial characteristics inside these bounding boxes are\n",
        "extracted.  In the second segment, short -term firing choices are ma de using LSTM\n",
        "networks that gather and analyz e spatial information over a brief period of time. At\n",
        "this point, smoke and flame are indistinguishable. This action simulates a human\n",
        "observer's fleeting decision -making process.  A majority voting procedure is used in\n",
        "the last part to integrate the short -term decisions into a l ong-term firing decision.\n",
        "Furthermore, the dynamic alterations in the SRoF areas, such as smoke and flame, are\n",
        "observed to offer insights into the changing nature of fire behavior. The tec hnique is\n",
        "excellent in guaranteeing strong SR oF detection and capturing the context data\n",
        "surrounding the bounding boxes.  Bounding boxes for flames, smoke, and non -fire\n",
        "regions are mostly provided by the Faster R -CNN object detection model;\n",
        "nevertheless, problems can develop when non -fire items mistakenly get detected as\n",
        "fire. These difficulties are lessened, meanwhile, by the deep object detection model's\n",
        "vast context data collection.  The approach's merits are in its capacity to closely mimic\n",
        "human decision -making by taking into account the temporal a nd dynamic\n",
        "characteristics of fire; nevertheless, its drawbacks include the requirement for\n",
        "meticulous hyperparameter tuning and a possible reliance on the length of majority\n",
        "voting in order to enhance accuracy .[4]\n",
        "\n",
        "A novel method utilizing Vision Transformer (ViT) technology ha s arisen in response\n",
        "to the increasing demand for quick and accurate fire monitoring in security video\n",
        "systems. ViT converts images into a series of patches as opposed to conventional\n",
        "Conv olutional Neural Networks (CNNs), which treat  every image pixel equally. This\n",
        "enables context -based selective attention to certain regions of the image. The ViT's\n",
        "attention mechanism solves the problems caused by tiny flames and allows for early\n",
        "fire detection. A refined Swin Transformer architecture, which effectively com putes\n",
        "self-attention with local windows and is especially useful for high -resolution images,\n",
        "is employed to maintain detection speed. The Vision Transformer outperformed\n",
        "existing state -of-the-art techniques with an outstanding 98.54%  classification\n",
        "accuracy in image fire dataset experiments, showing promising outcomes.[5]\n",
        "\n",
        "Many classifiers were contrasted in the experiments carried out to assess the\n",
        "suggested approaches. The baseline classifiers, CNN -Raw and SVM -Raw, have\n",
        "accuracy rates of roughly 93.1% and  92.2% on the training set, respectively. They\n",
        "were trained from scratch using raw pixel data. In terms of detection rate, the CNN\n",
        "classifier fared better than the SVM, but it had a little  higher false alarm rate. A full -\n",
        "image CNN cl assifier that was fine -tuned showed amazing accuracy of up to 100% in\n",
        "testing and training datasets. In comparison to raw pixel classifiers, linear SVM and\n",
        "non-linear neural network (NN) classifiers achieved higher accuracy rates when\n",
        "employing Pool -5 features. On the other hand , NN -Pool5 computed at a slower rate —\n",
        "1.4 seconds per image, as opposed to 0.16 seconds for SVM -Raw. Although the\n",
        "complete picture CNN classification took the longest, real -time fire detect ion systems\n",
        "could benefit from GPU implementa tion's considerable speed gain. Because NN -\n",
        "Pool5 regularly beat the other classifiers, the qualitative results were in favor of it. All\n",
        "in all, the strategies showcased attain excellent precision; however, several classifiers\n",
        "require longer computation times as a cost .[6]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. Dataset Overview :\n",
        "\n",
        "Earlier efforts in this discipline concentrated on curating databases that mostly\n",
        "featured images from specific categories or locales. The goal of our research was to\n",
        "create a comprehensive dataset of fire -related pictures from various locations,\n",
        "including both indoor and outdoor scenarios. To accomplish this, we used web\n",
        "scraping techniques to gather a large quantity of images. The images were obtained\n",
        "systematically from a dedicated website using carefully chosen keywords linked wi th\n",
        "both fire -related and non -fire settings. This method enabled us to create a dataset that\n",
        "gives a comprehensive and more diverse representation of fire -related imagery, laying\n",
        "the ground work for the research described in this paper . Following which, the scraped\n",
        "dataset was carefully inspected to identify and eliminate duplicate photos while also\n",
        "using an efficient manual curation process to remove any images that were judged\n",
        "inappropriate or unrelated to our research aims. This careful data cleaning and\n",
        "curation process ensures the quality and relevance of the dataset, setting the\n",
        "foundations for subsequent analysis and investigations in this study. A handpicked\n",
        "collection of 6,668 photos depicting fire incidents and 6,109 images dep icting non -\n",
        "fire events is used in this study. Attempts were made to train the model to prevent\n",
        "false alarms by including photos of smoke -like and fire -like images in the non -fire\n",
        "class such as images of fog, glare lights, street lights, sunset, etc. Following\n",
        "collection, rigorou s pre -processing and augmentation operations were meticulously\n",
        "carried out in accordance with the model employed in the paper.  The cleaned datasets\n",
        "were split into three categories: for te sting, validation, and training. 10% each of the\n",
        "classes was used for testing and validation, and the remaining 80% was used for\n",
        "training the model. Table 1 displays the distribution of image in the training, testing,\n",
        "and validation dataset.\n",
        "\n",
        "\n",
        "Table 1:  Data Description\n",
        "Dataset  Fire Non-fire Total\n",
        "Training  5334  4887  10221\n",
        "Testing  668 612 1280\n",
        "Validation  666 6n10  1276\n",
        "Total  6668  6109  12777\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Figure 1 . Sample images form Fire and Non -fire classes respectively used in this paper .\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4.   Methods and Models:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Figure 2 : Archit ecture of the Proposed Model\n",
        "\n",
        "\n",
        "\n",
        "4.1 CNN -LSTM hybrid model:\n",
        "\n",
        "The fusion of Convolutional Neural Networks (CNNs) and Long Short -Term Memory\n",
        "networks (LSTMs) in fire detection represents a sophisticated synergy of spatial and\n",
        "temporal information processing. CNNs, renowned for their prowess in discerning\n",
        "intricate spatial features within images, seamle ssly unveil critical elements such as\n",
        "edges, textures, and intricate patterns crucial for the discrimination between  fire and\n",
        "non-fire imagery.\n",
        "\n",
        "Enhancing precis ion is a paramount objective in fire detection, with the imperative to\n",
        "minimize the occurrence of false positives. Here, the LSTM plays a pivotal role by\n",
        "virtue of its capacity to contextualize and consider the historical progression of\n",
        "detections. This dynamic capability allows for the correction of false alarms; for\n",
        "example, in the event of smoke detection in one frame followed by its transient\n",
        "absence in subsequent frames, the LSTM brings its correctional infl uence to bear.\n",
        "\n",
        "In the realm of fire detectio n, diversity in lighting and environmental conditions poses\n",
        "a formidable challenge. The CNN -LSTM architecture emerges as a robust solution,\n",
        "adept at deciphering fire patterns across an array of scenarios by virtue of its ability to\n",
        "process sequences of images. For non -sequence of images, each image is taken as a\n",
        "separate sequence. This amalgamation of deep learning techniques not only ensures\n",
        "the capture of diverse fire manifestations but also empowers the model with a level of\n",
        "adaptability and sophisticati on that is instrumental in addressing the multifaceted\n",
        "nature of fire detection.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4.2 Ensemble of Xception and MobileNetV2:\n",
        "\n",
        "In this paper we implemented an ensemble of two transfer learning models, Xception\n",
        "and MobileNetV2 for classification of images as the individual classification accuracy\n",
        "of the pre -trained Xception and MobileNetV2 models was o nly 60 leaving room for\n",
        "improvement. These two weak classifiers is ensembled into a strong classifier using\n",
        "the ense mble technique of averaging the pre -trained Xception and Mobile NetV2\n",
        "models were combined with specially created output layers for binary classification\n",
        "the ensemble model integrates the results from the two models effectively by utilizing\n",
        "the variety of each models advantages and disadvantages this tactical ensemble\n",
        "strategy significantly improved the classification tasks accuracy which rose to an\n",
        "astounding 87 demonstrating the value of model ensembling in enhancing machine\n",
        "learning systems overall perfor mance.\n",
        "\n",
        "We used the feature extraction capab ilities of two transfer learning models —Xception\n",
        "and MobileNetV2 —that have been pre -trained on large image datasets in our study.\n",
        "MobileNetV2 is superior in efficiency and generalization, while Xception is best at\n",
        "capturing fine spatial information. From our dataset, they collectively extracted rich\n",
        "feature representations. Our ensemble technique, w hich combined the feature vectors\n",
        "from Xception and MobileNetV2, was the main source of innovation. We were able\n",
        "to create a harmonious balance by averaging thei r forecasts, which improved our\n",
        "classification accuracy and reduced model biases.\n",
        "\n",
        "We used common image classification metrics such as accuracy, precision, recall, F1 -\n",
        "score, and ROC -AUC to assess the performance of our ensemble model. These\n",
        "metrics offered a thorough evaluation of our model's ability to discriminate between\n",
        "images with and without f ire.\n",
        "\n",
        "Essentially, our work provides a useful ensemble model that combines the feature\n",
        "extraction powers of MobileN etV2 and Xception. We achieved amazing image\n",
        "classification performance, especially in the critical domain of fire detection, by\n",
        "strategically utilizing ensemble approaches. This methodology exhibits potential for a\n",
        "multitude of practical uses, such as augmenting security and safety.\n",
        "\n",
        "Although our ensemble model has shown encouraging improvement in improving the\n",
        "accuracy of picture categorizat ion, it is important to understand the limits of the\n",
        "individual Xception and MobileNetV2 models. Due to the sensitiv ity of these models\n",
        "to the quality and divers ity of the data, when input photos deviate greatly from\n",
        "training data, accuracy may be compromised. This problem is particularly relevant in\n",
        "real-world situations where there are variations in image quality because to things like\n",
        "lighting and artifacts. Moreover, our tests showed that these models might not\n",
        "perform well in situations with complicate d backdrops, which could affect their\n",
        "accuracy. Post -processing techniques are frequently needed for binary classifi cation\n",
        "applications, such as fire detection, in order to refine results and impact model\n",
        "performance. Our study demonstrates the benefits and drawbacks of our group\n",
        "method, emphasizing the necessity of customizing solutions for particular use cases\n",
        "and resolving issues with data pre -processing, model optimization, and post -\n",
        "processing. Since there isn't a single, perfect method for classifying images, our\n",
        "research recommends investigating stronger models or hybrid strategies in the future.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5. Results and D iscussions :\n",
        "\n",
        "Table 2: Classification report o f the CNN -LSTM  hybrid model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Figure 3 : Confusion matrix for the CNN -LSTM hybrid model\n",
        "\n",
        " From the confusion matrix  Figure 3, out of 1280 test ca ses the CNN -LSTM  hybri d\n",
        "model  has correct ly detect ed 611 fire ima ges and 567 non fire images , thus giving an\n",
        "accuracy of 87% . But it has fa iled detect  57 fire ima ges and misclassified  detected 45\n",
        "non fire images  which amou nts to Type II and Type I error respe ctively. We must aim\n",
        "to prevent Type II error as it leads to disast rous eve nts, as compared to  Type I error\n",
        "which leads to false alarms . The classification report  Table 2  supports the above claim\n",
        "with the performance  metrics of the model depict ing precision score  of 0.87 , recall\n",
        "score of 0.88 , and f1 score of 0.88\n",
        "\n",
        "Table 3. Classification report for the ensemble of Xception and MobileNetV2\n",
        " Type of Image  Precision  Recall  F1 score  Support\n",
        "Fire 0.87 0.88 0.88 666\n",
        "Non-Fire 0.87 0.86 0.86 610\n",
        "Type of Image  Precision  Recall  F1 score  Support\n",
        "Fire 0.93 0.91 0.92 668\n",
        "Non-Fire 0.91 0.93 0.92 612\n",
        "\n",
        "\n",
        "Figure 4:  Confusion matrix for ensemble of Xception and MobileNetV2.\n",
        "\n",
        "From the confusion matrix  Figure 4 , out of 970 test ca ses the ensemble  model  has\n",
        "correct ly detect ed 660 fire ima ges and 310 non fire images , thus giving an accuracy of\n",
        "92%. But it has fa iled detect  62 fire ima ges and misclassified  detected 4  non fire\n",
        "images  which amou nts to Type II and Type I error respectively. We must aim to\n",
        "prevent Type II error as it leads to disast rous eve nts, as compared to  Type I  error\n",
        "which leads to false alarms . The class ification report supports the above claim with\n",
        "the performance  metrics of the model depict ing precision score  of 0.93, recall score of\n",
        "0.91, and f1 score of 0. 92. The number images that are failed to detect is 4 which is\n",
        "less than the CNN -LSTM model , thus Type II error is also less henc e the Ensemble\n",
        "model is better than the CNN -LSTM model.\n",
        "\n",
        "The CNN -LSTM hybrid model achieved an impressive accuracy rate of 92%. The\n",
        "model also gave a high F1 -score as shown in Tab le 2. The confusion matrix in Figure\n",
        "3 shows a low number of false positives and false negative count, thus reduced false\n",
        "alarms.\n",
        "Our experimental data shows that our ensemble model outperforms individual models\n",
        "consistently. It improved accuracy while simultaneously lowering the likelihood of\n",
        "false positives, making it a reliable tool for detecting fire in image data. The\n",
        "classification  report shows a high F1 -score depicting consistent performance of the\n",
        "model  (Table 3). The confusion matrix in Figure 4 sh ows a low number of false\n",
        "positive and false negative count implying a reduced false alarm.\n",
        "\n",
        "6. Conclusion :\n",
        "\n",
        "Our study tackles the crucial problem of fire detection in diverse settings,\n",
        "emphasizing the risk to both human life and expensive assets. We've created learning\n",
        "models for fire detection, including CNN -LSTM hybrid model, with success.\n",
        "Furthermore, our work supports the creation of an ensemble of deep learning models\n",
        "for fire detection by combining the Xception and MobileNetV2 architectures.\n",
        "Experim ents show that our suggested CNN -LSTM hybrid model has an amazing\n",
        "accuracy rate of 92%, consistently outperfor ming both state -of-the-art methods and\n",
        "current models. In a variety of contexts, this research helps to improve asset\n",
        "protection and fire safety.  An ensemble of Xception, ResNet50, Inception gave a very\n",
        "good accuracy of 94% and prediction among  all the models.\n",
        "\n",
        "People can also try using different models and Ensemble them for the good accuracy.\n",
        "As a part of Literature Review many of implemented vide o-based prediction and\n",
        "ensemble. Others can a lso try Alarm implementation along with models\n",
        "\n",
        "References\n",
        "1. Veerapp ampalayam Easwaramoorthy Sathishkumar, Jaehyuk Cho, Malliga ,\n",
        "Subramanian & Obuli Sai Naren: Forest fire and smoke detection using deep\n",
        "learning -based learning without Forgetting Fire Ecology volume 19, Article\n",
        "number: 9 (2023)\n",
        "2. Jivitesh Sharma, Ole -Christoffer Granmo, Morten Goodwin & Jahn Thomas Fidje:\n",
        "Deep Convolutional Neural Networks for Fire Detection in Images - First Online\n",
        "(2017)\n",
        "3. Abdulaziz NAM OZOV, Young Im CHO: An Efficient Deep Learnin g Algorithm for\n",
        "Fire and Smoke Detection with Limited Data (2018)\n",
        "4. Seyd Teymoor Seydi, Vahideh Saeidi, Bahareh Kalantar, Naonor i Ueda, and Alfian\n",
        "Abdul Halin: Fire -Net: A Deep Learning Framework for Active Forest Fire\n",
        "Detection (2022)\n",
        "5. Renjie Xu, Haifeng Lin, Kangjie Lu, Lin CaoandYunfei Liu:  A Forest Fire Detection\n",
        "System Based on Ensemble Learning (2021)\n",
        "6. Deni Sutaji a c, Oktay Yıldız b - LEMOXINET: Lite ensemble MobileNetV2 and\n",
        "Xception models to predict plant dis ease. ( 2022)\n",
        "7. Manh Dung Nguyen; Hoai Nam Vu; Duc Cuong Pham; Bokgil Choi: Multistage\n",
        "Real-Time Fire Detection Using Convolutional Neural Networks a nd Long\n",
        "Short -Term Memory  Networks (2022)\n",
        "8. Otabek Khudayberdiev, Jiashu Zhang, Ahmed Elkhalil & Lansana Balde:  Fire\n",
        "Detection Approach Based on Vision Transformer (2022)\n",
        "9. Khubab Ahmad, Muhammad Shahbaz Khan, Fawad Ahmed, Maha Driss, Wadii\n",
        "Boulila, Abdulwahab Alazeb, Mohammad Alsulami, Mohammed S. Alshehri,\n",
        "Yazeed Yasin Ghadi & Jawad Ahmad : FireXnet: an explainable AI -based\n",
        "tailored deep learning model for wildfire detection on resource -constrained\n",
        "devices (2023\n",
        "10.  Gabriel Henrique de Almeida Pereira, Andre Mino ro Fusioka, Bogdan Tomoyuki\n",
        "Nassu, Rodrigo Minetto : Active fire detection in Landsat -8 imagery: A large -\n",
        "scale dataset and a deep -learning study  (2021)\n",
        "11. Yavuz Selim Taspinar , Murat Koklu , Mustafa Altin : Fire Detection in Images Using\n",
        "Framework Based on Image Processing, Motion Detection and Convolutional\n",
        "Neural Network  (2021)\n",
        "12. Akmalbek Bobomirzaevich Abdusalomov , ORCID,Bappy MD Siful Islam , Rashid\n",
        "Nasimov ORCID,  Mukhriddin Mukhiddinov ORCID and  Taeg Keun Whangbo :\n",
        "An Improved Forest Fire Detection Method Based on the Detectron2 Model and\n",
        "a Deep Learning Approach  (2023)\n",
        "13. Pu Li a b, Wangda Zhao: AImage fire detection algorithms based on convolutional\n",
        "neural networks.  (2020)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Chunk the input text into segments\n",
        "segment_length = 1000  # Adjust segment length as needed\n",
        "segments = [text[i:i + segment_length] for i in range(0, len(text), segment_length)]\n",
        "\n",
        "# Generate summaries for each segment\n",
        "summaries = []\n",
        "for segment in segments:\n",
        "    summary = summarizer(segment, max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
        "    summaries.append(summary)\n",
        "\n",
        "# Concatenate the summaries of all segments\n",
        "final_summary = \" \".join(summaries)\n",
        "\n",
        "# Print the final summary\n",
        "print(final_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "70d69b27cc51489496b24ef17420579a",
            "aa68f43afb844a4b8e528c88b5d2b5c3",
            "c5cdc335948b4f94b82f7e5d0569c88e",
            "2498ba2aa1fd466e903f14305708b1aa",
            "19e31796143746cf9837c8faad70d05e",
            "2331bf62a8364773a31e60eea1088908",
            "353336ad0b5c4e738680fb482fb25552",
            "d7ac82be636b4421bd06d5186397be35",
            "3ae0fbc1628d46098017d16bcfe396e4",
            "e3fbaae2bf974fd989d7b82039b27fc6",
            "1641ee8871824fea93ee31e33b93840a",
            "3eaf085d27a144f68071229ccd296321",
            "f3c0f5e326c24417945110d7534a2aa8",
            "8f68fe0c857b44fc82813cc6e5b41a6a",
            "1c7856f8af5b4972b0bc3436f57ed5b9",
            "f7343b9120054ba89a792e4926e16246",
            "4fbfb6750dcd4e6a808fe15472c99f5c",
            "618137c0c2ef4f2c85c528b32fba5d16",
            "136197d14f6d4585ba8bd623aa2ed171",
            "1dbb599f05554b00a3c55da3348eddb5",
            "73df74e5fdc741ef9c937f8fa0c32be0",
            "c13ee32713f3424994f599d84d5ba7dd",
            "291a2aac0cdc4b04b00920bab6264195",
            "cf135fdf2c8d4631a808b5feb0b395f3",
            "1de8d27f1ecb44bbb24f8909c9a75f0a",
            "b58b02ecb4e84aef82646ef0c5e1da53",
            "c2122bfe494b4235956433c0d3a5a426",
            "45205801bceb4c768057d18f7303279a",
            "78ff0fe7a7a348909323bcb501c7cd73",
            "801cac2f51264417ad3c65d7ea1e3ac6",
            "b89b6d1218b74476ac15c316cd95a4a2",
            "8b982f9cac264427b65666a155f67116",
            "2156b39998924ea08b7d229746061ecc",
            "72c3ae60162747caade1cbd080ba8542",
            "a672974680f04055a3c6c6c3f3d9c065",
            "4413c9423b944fae8493a50bbfc784ac",
            "1f656c9e3b564b9bbc017ac1cb1c8f80",
            "e76ddba5ceb74b2ca58a88eb188db05a",
            "4187921a332140ffafe1145ee77e54a2",
            "181403172b5548758959117549ff595a",
            "a1dc2f7d605c43c7b50b6a3725de60aa",
            "ba35f0a3b7af4b0c96515ccd1aea09c6",
            "e2a7512d7acf4c03bb5adf0ed0b7977b",
            "0c41851fb7214317bef2b87bf374e95a",
            "644a910a88ff4bf4bbce188adbfdbd68",
            "acfe3d24e7e4456cb917ff1a24ea43d4",
            "80b7a8c4924f4bccb837042f5e0c763b",
            "2cc528144764444da10ea89f3cbd60ed",
            "bf63605e927f4b0f8b9542e5850796f8",
            "a1668209ddde42a3ba7104183194e0b7",
            "4111bb583ddb4f838f5df60a42525471",
            "2603feda5c4a49308dd9c1d3f0391e0d",
            "825a772a264d46ab836e415b220853ed",
            "1abc7cff59a5403d951d4e20d490eea3",
            "888da28c32d343b7b91830546dc0816f"
          ]
        },
        "id": "t5Zpf8GgSChm",
        "outputId": "65e5b52c-39e6-4e6d-e5c5-0b3600356a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70d69b27cc51489496b24ef17420579a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3eaf085d27a144f68071229ccd296321"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "291a2aac0cdc4b04b00920bab6264195"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72c3ae60162747caade1cbd080ba8542"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "644a910a88ff4bf4bbce188adbfdbd68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Researchers from St. Joseph’s University in Bengaluru have developed deep learning models that assist in detecting fires in various environments like forests, stores, commercial kitchens, meeting rooms, etc. They have successfully demonstrated the effectiveness of de ep lear ning models for fire detection using a computer .  The study suggests combining MobileNetV2 and Xception to form an ensemble of Convolutional Neural Networks (CNNs) for detecting fires . Training these models can be computationally intensive and require large datasets . The ensemble of Xception, ResNet50, I nception gave a best result of 94% .  The nature of fire accidents evolved alongside our understanding and mastery of fire,  reflecting the developing complexities of our societies and technologies . Deliberate acts of arson highlight the potentially destructive nature of human intent, which adds another level of intricacy to this age -consuming problem .  Heat sensors, smoke detectors, and fire supp ression systems are important examples of these . Smoke detectors are essential for detecting fires, however they can . However, they can malfunction in low -smoke or slow -burning fires . Their placement also affects how efficient they are .  Heat sensors are made to detect tem peratu re spikes  during fires . Slow -burning fires may be difficult for heat sensors to detect, especially  early phases of the fire . AI surveillance is flexible for threat identification since, in  contrast to traditional sensors, it is highly proficient in identifying a broad range of  behaviours, behaviours, and anomalies [ 9].  AI -enabled surveillance overcomes the drawbacks of previous techniques to provide a more flexible, effective, and all-encompassing approa ch to security and surveillance . These systems can monitor vast regions effectively due to the scalability and the ability to analyse data on their own .  The CNN -LSTM hybrid model combines Convolutional Neur al Networks with Long Short -Term Memor y networks . The accuracy of fire detection could be improved by using this method .  Data from Google, Kaggle, satellite imagery, and the BoWFire dataset to create a dataset of 6,911 photos of smoke and forest fires for their study . VGG16 reached 93.2% accuracy, InceptionV3 at 94.5%, and InceptionXception leading at 95.1% .  The results show that adding fully connected layers for fine -tuning increases accuracy, even though it requires more training time . This paper presents a novel approach employing even deeper CNNs, VGG16 and Resnet50, optimized with completely connected layers .  Fire -Net is a deep learning system built to identify current fires from Landsat -8 imagery . Researchers used a two -stream pixel classification approach with a distinctive design to improve the detection performance .  A novel deep learning -based fire detection method called DTA (Decision Through Aggregation) was suggested in this study . The robustness and efficacy of this method in identifying small fires are its advantages; however, the need for meticulous hyperparameter tuning and extended training periods are possible disadvantages .  Suspected regions of fire (SRoFs) and non -fire objects are distinguished in video frames using the Faster R -CNN deep object detection model . In the second segment, short -term firing choices are made using LSTM networks that gather and analyz e spatial information over a brief period of time .  Bounding boxes for flames, smoke, and non -fire regions are mostly provided by the Faster R -CNN object detection model . The approach's merits are in its capacity to closely mimic  human decision -making by taking into account the temporal a nd dynamic  characteristics of fire . However, its drawbacks include the requirement for hyperparameter tuning .  Vision Transformer outperformed state -of-the-art techniques with an outstanding 98.54%  classification accuracy in image fire dataset experiments, showing promising outcomes . The ViT's attention mechanism solves the problems caused by tiny flames and allows for early fariousfire detection .  A full -image CNN cl assifier that was fine -tuned showed amazing accuracy of up to 100% in testing and training datasets . CNN classification took the longest, real -time fire detect ion systems could benefit from GPU implementa tion's considerable speed gain .  The goal of our research was to create a comprehensive dataset of fire -related pictures from various locations . The images were obtained systematically from a dedicated website using carefully chosen keywords linked to specific locations . Following which, the scraped dataset was carefully inspected to identify and eliminate duplicate photos .  A handpicked collection of 6,668 photos depicting fire incidents and 6,109 images dep icting non -fire events is used in this study . Attempts were made to train the model to prevent false alarms by including photos of smoke -like and fire -like images in the non-fire images . The cleaned datasets were split into three categories: for sting, validation, and training .  The fusion of Convolutional Neural Networks (CNNs) and Long Short -Term Memory (LSTMs) in fire detection represents a sophisticated synergy of spatial and temporal information processing . CNNs, renowned for their prowess in discerning spatial features within images, seamle ssly unveil critical elements such as  grotesqueedges, textures, and intricate patterns crucial for the discrimination between fire and non-fire imagery .  CNN -LSTM architecture emerges as robust solution to deciphering fire patterns across an array of scenarios by virtue of its ability to  process sequences of images . For non -sequence of images, each image is taken as a  separate sequence .  Ensemble of Xception and MobileNetV2 models is ensembled into a strong classifier using the ensese mble technique of averaging the pre -trained Xception  and Mobile NetV2 . The ensemble model integrates the results from the two models effectively by utilizing the variety of each models .  MobileNetV2 is superior in efficiency and generalization, while Xception is best at capturing fine spatial information . The ensemble technique combined the feature vectors of Xception and MobileNet V2, was the main source of innovation .  An ensemble model has shown encouraging improvement in improving the accuracy of picture categorizat ion . But it is important to understand the limits of the Xception and MobileNetV2 models . Post-processing techniques are frequently needed for bina .  The CNN -LSTM hybrid model has an accuracy of 87% . It has correct ly detected ed 611 fire ima ges and 567 non fire images . But it has misclassified  detected 45  non fire images which a misclassified .  We must aim  to prevent Type II error as it leads to disast rous eve nts, as compared to Type I error which leads to false alarms . The classification report  Table 2 supports the above claim . The performance  metrics of the model depict ing precision score  of 0.87 .  The CNN -LSTM hybrid model achieved an impressive accuracy rate of 92%. The Ensemble model outperforms individual models . It improved accuracy while simultaneously lowering t t . The confusion matrix shows a low number of false positives and false negative count .  The CNN -LSTM hybrid model has an amazing accuracy rate of 92%, consistently outperforming both state -of-the-art methods and current models . The confusion matrix in Figure 4 sh ows a low number of false positive and false negative count implying a reduced false alarm .  An ensemble of Xception, ResNet50, Inception gave a very  good accuracy of 94% and prediction among all the models . People can also try using different models and Ensemble them for the good accuracy .  FireXnet: an explainable AI -based tailored deep learning model for wildfire detection on resource -constrained . Researchers: Forest Fire Detection Using Convolutional Neural Networks .  An improved Forest Fire Detection Method based on the Detectron2 model and a deep-learning approach . An improved forest fire detection method based on a deep -learning approach. evices .\n"
          ]
        }
      ]
    }
  ]
}